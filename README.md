# Paper-Implementation-Overview-Gradient-Descent-Optimization-Algorithms  

[![forthebadge made-with-python](http://ForTheBadge.com/images/badges/made-with-python.svg)](https://www.python.org/) 


## arXiv paper : "An Overview of Gradient Descent Optimization Algorithms"   
##  - Sebastian Ruder  

## Python 2.7  

Links to original paper published on arXiv.org>cs>arXiv:1609.04747  : [[1]](https://arxiv.org/abs/1609.04747), [[2]](https://arxiv.org/pdf/1609.04747.pdf)   

Link to Blog with Paper Explanation : [[3]](http://ruder.io/optimizing-gradient-descent/index.html)  

Implemented following Gradient Desent Optimization Algorithms from Scratch:  

1. Vanilla Batch/Stochastic Gradient Descent [[4]](https://en.wikipedia.org/wiki/Stochastic_gradient_descent)   

2. Momentum [[5]](https://www.cs.toronto.edu/~fritz/absps/momentum.pdf)  
3. NAG : Nesterov Accelarated Gradient  [[6]](https://www2.cs.uic.edu/~zhangx/teaching/agm.pdf)
4. AdaGrad : Adaptive Gradient Algorithm [[7]](http://www.jmlr.org/papers/volume12/duchi11a/duchi11a.pdf)
5. AdaDelta : Adaptive Learning Rate Method [[8]](https://arxiv.org/abs/1212.5701)
6. RMS Prop  [[9]](https://www.cs.toronto.edu/~tijmen/csc321/slides/lecture_slides_lec6.pdf)   
7. Adam : Adaptive Moment Estimation [[10]](https://arxiv.org/abs/1412.6980) [[11]](https://arxiv.org/pdf/1412.6980.pdf)  
8. AdaMax : Infinity Norm based Adaptive Moment Estimation [[12]](https://arxiv.org/pdf/1412.6980.pdf)  
9. Nadam : Nesterov-accelarated Adaptive Moment Estimation [[13]](http://cs229.stanford.edu/proj2015/054_report.pdf)  
10. AMSGrad [[14]](http://www.satyenkale.com/papers/amsgrad.pdf) 
